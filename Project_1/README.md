# Project 1

**Topic**: Multilayer perceptron (MLP) employing backpropagation algorithm

## Datasets

* project part I - syntetic dataset provided druing the class
* project part II - MNIST dataset (https://www.kaggle.com/c/digit-recognizer)

## Requirements

* low-level implementation - using only some basic packages like NumPy, pandas, etc.
* MLP parameters are:
  * number of hidden layers and number of neurons in hidden layers
  * activation function
  * bias presence
  * batch size
  * number of iterations
  * learning rate
  * momentum
  * problem type: classification or regression
• during the project presentation - training and testing implemented network on new (unseen before) datasets
• reproducibility by initializing a random number generator with a constant seed
* ploting training and test error
* tracking learning process iteration by iteration (visualization of edges’ weights) as well as a propagated error (visualization of an error on each edge)
* visualization of a training set and classification/regression result (as a background)

## Elements to analyze
* How does activation function affect the model’s accuracy? Experimenting with sigmoid and two other activation functions. The activation function in an output layer should be chosen accordingly to the problem.
• How does the number of hidden layers and number of neurons in hidden layers impact the model’s accuracy? Analizing different architectures.
• How does the loss function affect the model’s accuracy? Considering two different loss functions for both classification and regression.

## Timetable

* 02.03 - Tutorial hours
* 09.03 - Initial presentation of the part I
* 16.03 - Project 1, part I deadline
* 23.03 - Project 1, part II deadline

## Useful resources
* https://www.deeplearningbook.org
* http://neuralnetworksanddeeplearning.com/
* https://www.coursera.org/specializations/deep-learning
* https://drive.google.com/drive/folders/0B5DSlxnH–fzR1hxd3VYOUhua2c

